{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing text files exported from Web of Science Database and Compiling them into a single dataset (and saving as a .csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import files if using Google Colab\n",
    "\n",
    "If using Colab, uncomment out the cell below and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Option 1:\n",
    "#!wget https://git.dartmouth.edu/lib-digital-strategies/RDS/projects/bibliometrics/-/archive/main/bibliometrics-main.zip\n",
    "#!unzip bibliometrics-main.zip\n",
    "\n",
    "##Option 2:\n",
    "#!git clone https://git.dartmouth.edu/lib-digital-strategies/RDS/projects/bibliometrics.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path   #for working with filepaths\n",
    "import pandas as pd        #for creating and working with dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set path to directory containing WoS Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check your current working directory\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathdir = Path.cwd()                        #use this if tsv .txt files are in current working directory\n",
    "pathdir = Path(\"../data/resilience/orig_txt\")                 #use this to set relative path to .txt files if they are not in cwd\n",
    "\n",
    "#pathdir = Path(\"WoS/resilience\")         \n",
    "pathlist = sorted(pathdir.glob('*.txt'))    \n",
    "[path.name for path in pathlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in tab-separated-value text files and combine into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []                           #creates an empty list\n",
    "for i, path in enumerate(pathlist):\n",
    "    df = pd.read_csv(path, sep = \"\\t\")\n",
    "    print(\"Reading & appending file number:\", i, \"(with %s rows) of\" %df.shape[0], len(pathlist), \"total files. Pathname: \", path.stem)\n",
    "    datalist.append(df)    ## appends each imported dataframe into a list of dataframes\n",
    "    \n",
    "data = pd.concat(datalist)        #concatenates or joins each dataframe in datalist into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get summary information for the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "# can also try:\n",
    "## data.describe()\n",
    "## data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export full dataframe into a .csv file\n",
    "\n",
    "Skip to step #6 if you only want to export a subsetted version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = Path(\"../data/resilience/merged\")\n",
    "data.to_csv(Path(outputdir,\"merged_wos_files.csv\"), encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subset dataframe and then export\n",
    "\n",
    "Often, the Web of Science database provides more data fields than we need. We can work with a smaller version of the dataset by only keeping those columns we really want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the data fields (columns in this case) available for Web of Science data. You can see a [full List of WoS data fields here.](https://docs.google.com/spreadsheets/d/1KPNVIrhwZJrqYOsu3jzpRF7pzHCjRVy6K8eu3qTu-cA/edit#gid=1397269035) \n",
    "\n",
    "Then, choose which columns you would like to keep by placing the two-letter column name in the list below.\n",
    "\n",
    "*Place this information somewhere public so I don't have to link to an institutional drive!!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"PT\", #pub type\n",
    "                \"AU\", \"AF\", #author / author full names\n",
    "                \"TI\",  #author title\n",
    "                \"SO\",  #source title\n",
    "                \"LA\",  #language\n",
    "                \"DT\",   #document type\n",
    "                \"DE\", \"ID\",  #author keywords / keywords plus\n",
    "                \"AB\",      #abstract\n",
    "                \"RI\", \"OI\",  #research ids / ORCIDs\n",
    "                \"CR\", #cited references\n",
    "                \"TC\", \"Z9\", \"U1\", \"U2\", #times cited (WoS core) / times cited, all / 180 days usage ct / since 2013 usage count\n",
    "                \"HC\", \"HP\", #highly cited status / hot paper status\n",
    "                \"PU\",  #publisher\n",
    "                \"SN\", \"EI\", \"BN\", \"DI\", \"UT\",   #ISSN / eISSN / ISBN / DOI / WoS id\n",
    "                \"JI\", #journal ISO abbreviation\n",
    "                \"PD\", \"PY\",   #pub data / pub year\n",
    "                \"WC\", \"SC\"  #WoS Categories / Research Areas\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a new dataframe (\"subdata\") with only the columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata = data.loc[:, cols_to_keep]\n",
    "print(subdata.shape)   #print the new dimensions of the dataframe\n",
    "subdata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the dataframe to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.to_csv(Path(outputdir, \"merged-wos_subcols.csv\"), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create a random sample and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand1000 = subdata.sample(n = 500)\n",
    "rand1000.to_csv(Path(outputdir, \"merged_wos_rand500.csv\"), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
